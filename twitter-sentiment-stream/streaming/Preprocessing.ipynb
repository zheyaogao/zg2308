{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pickle\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo netstat -tlnp|grep 5050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "import sys \n",
    "import requests\n",
    "import re\n",
    "import socket\n",
    "import time as time1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "dataStream=ssc.socketTextStream(\"localhost\",9009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['#louwilliams', '#stephencurry', '#kevindurant', '#cousins', 'williams', 'curry', 'durant', 'demarcus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCP_IP = \"localhost\"\n",
    "TCP_PORT = 5050\n",
    "conn = None\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "s.bind((TCP_IP, TCP_PORT))\n",
    "s.listen(1)\n",
    "print(\"Waiting for TCP connection...\")\n",
    "conn, addr = s.accept()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gethash(tweet_text):\n",
    "    words = tweet_text.split()\n",
    "    hashtags = [w.lower() for w in words if w.lower() in tags]\n",
    "    hashtags = [[w,tags[tags.index(w)-4]][tags.index(w)>3] for w in hashtags]\n",
    "    return True if hashtags else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(original_tweet_text):\n",
    "    words = original_tweet_text.split()\n",
    "    hashtags = [w.lower() for w in words if w.lower() in tags]\n",
    "    hashtags = [[w,tags[tags.index(w)-4]][tags.index(w)>3] for w in hashtags]\n",
    "    hashtags = ' '.join(hashtags)\n",
    "    tweet_text = ' '.join([words, words[1:]][words[0]=='RT'])\n",
    "    tweet_text=re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', tweet_text, flags=re.MULTILINE)\n",
    "    tweet_text=re.sub('@[\\w]*', ' ', tweet_text)\n",
    "    tweet_text=re.sub('#[\\w]*', ' ', tweet_text)\n",
    "    tweet_text=re.sub('[^A-Za-z0-9]+', ' ', tweet_text)\n",
    "    return (hashtags,original_tweet_text,tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_context_instance(spark_context):\n",
    "    if('sqlContextSingletonInstance'not in globals()):\n",
    "        globals()['sqlContextSingletonInstance']=SQLContext(spark_context)\n",
    "    return globals()['sqlContextSingletonInstance']\n",
    "\n",
    "def process_rdd(time,rdd):\n",
    "    print(\"-----------%s-----------\"%str(time))\n",
    "    try:\n",
    "        sql_context=get_sql_context_instance(rdd.context)\n",
    "        row_rdd = rdd.map(lambda w: Row(hashtag=w[0],original=w[1],text=w[2]))\n",
    "        df = sql_context.createDataFrame(row_rdd)\n",
    "    except:\n",
    "        e=sys.exc_info()[0]\n",
    "        print(e)\n",
    "    finally:\n",
    "        try:\n",
    "            if row_rdd.take(1):\n",
    "                df1 = df.toPandas()\n",
    "                #msg = (df1['hashtag'].values,df1['text'].values)\n",
    "                #msg = (df['hashtag'].values,df['text'].values)\n",
    "                #print(msg)\n",
    "                start = time1.time()\n",
    "                for name, original, text in df1.values:\n",
    "                    msg = {'name':name,'original':original,'text':text,'time':start}\n",
    "                    #print(msg)\n",
    "                    print('conn ready')\n",
    "                    conn.send(pickle.dumps(msg))\n",
    "                    print('conn successful')\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split each tweet into words\n",
    "#words=dataStream.flatMap(lambda line:line.split(\" \"))\n",
    "words = dataStream.filter(gethash).map(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.foreachRDD(process_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start the streaming computations\n",
    "ssc.start()\n",
    "#wait for the streaming to finish\n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
